{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "In this tutorial, we will build a convolutional neural network.\n",
    "\n",
    "In short, the differences to a feedforward neural network are not too big, the difference is that with images we usually have a large input space (width x length) and the spatial position _matters_. We would not recognize an image whose pixels are randomly shuffled.\n",
    "\n",
    "Therefore, convolutions and pooling is used. This corresponds to a filter (\"mini image consisting of 3x3 - 5x5 pixels) scanning over the whole image and returning a value proportional to \"how well the mini image fits\".\n",
    "\n",
    "For a more in depth explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import FileLink, FileLinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get our data that is stored in the cloud. Using the link inside a browser, we can also download the data to our local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the data\n",
    "!wget \"https://drive.switch.ch/index.php/s/xcLDFKJAyyZGmfD/download\" -O train_img.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data and some dimensional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 image has 40x40 pixels = 1600 pixels\n",
    "pixels = [\"img_{0}\".format(i) for i in range(1600)]\n",
    "\n",
    "def to_image(df):\n",
    "    return  np.expand_dims(np.expand_dims(df[pixels], axis=-1).reshape(-1,40,40), axis=-1)\n",
    "\n",
    "\n",
    "# Read the first 10k events\n",
    "store_train = pandas.HDFStore(\"train_img.h5\")\n",
    "df_train = store_train.select(\"table\")\n",
    "images_train = to_image(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple helper function\n",
    "def plot_image(number):\n",
    "    plt.imshow(images_train[number, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the images that we gonna train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADW1JREFUeJzt3W+sZHV9x/H3Z+/ushYhsPzLFqhSS1tMU9eUUlr7wKLWLU/AVBNJ2mBKom1Ko4lppD5RmpJootKkbUwwUraJFYl/Cmmo7WaLsaZ2BXFFcLUgoq5sdrVCxCYu7O63D+Zsst07F2Zn5sydmd/7lUxmzm/O3PM92fvZM/O7Z843VYWk9mxY7wIkrQ/DLzXK8EuNMvxSowy/1CjDLzXK8EuNMvxSoyYKf5IdSb6Z5LEkN02rKEn9y7hn+CVZAf4beB2wH7gfuK6qvr7WazbntNrC6WNtT9IL+yn/y7N1OKOsu3GC7VwBPFZVjwMkuRO4Blgz/Fs4nd/IaybYpKTns6d2j7zuJG/7LwS+d8Ly/m5M0gKY5Mg/7K3Fqs8QSd4KvBVgCz8zweYkTdMkR/79wMUnLF8EPHnySlV1W1VdXlWXb+K0CTYnaZomCf/9wKVJLkmyGXgzcM90ypLUt7Hf9lfVkSQ3Av8KrAC3V9UjU6tMUq8m+cxPVd0L3DulWiTNkGf4SY0y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UqImu4ZfkCeAZ4ChwpKoun0ZRGkPW6NA0Zjs2Lb+Jwt/5nar64RR+jqQZ8m2/1KhJw1/AvyX5cteWa5Ukb03yQJIHnuPwhJuTNC2Tvu1/VVU9meR8YFeSb1TV509coapuA24DODNb/QAqzYmJjvxV9WR3fwj4DIO23ZIWwNjhT3J6kjOOPwZ+F3h4WoXNUjZuHHrrxYaV4bdJVQ2/SWuY5Df8AuAzGfyJaSPwj1X12alUJal3kzTqfBx4xRRrkTRD/qlPapThlxrV06zWYqkjR2a3sWNHZ7etvqw1QbkM+9YQj/xSowy/1CjDLzXK8EuNMvxSo5zt16lzVn8peOSXGmX4pUYZfqlRhl9qlBN+mp5hVxD2mgJzyyO/1CjDLzXK8EuNMvxSo14w/EluT3IoycMnjG1NsivJo9392f2WqYXgBUQXyihH/juAHSeN3QTsrqpLgd3dsqQF8oLh75pw/Oik4WuAnd3jncC1U65LUs/G/cx/QVUdAOjuz19rRdt1SfOp9wm/qrqtqi6vqss3cVrfm5M0onHDfzDJNoDu/tD0SpI0C+Oe3nsPcD3wvu7+7qlVtOSyafPQ8Xru2RlXstqGM85YNXbsmWfWoRLNwih/6vs48EXgl5LsT3IDg9C/LsmjwOu6ZUkL5AWP/FV13RpPvWbKtUiaIc/wkxpl+KVGLcf3+Rfoe+TzMLG3lnmd3Fs577xVY0d/8IN1qGS5eOSXGmX4pUYZfqlRhl9qlOGXGrUcs/1zOrM/SxsvecnQ8SPf/s6MK5k+Z/b74ZFfapThlxpl+KVGGX6pUcsx4bdAVs7ZOnT86P+cfJnEU9PXxN6P/ug3V41tvf2LvWxLs+WRX2qU4ZcaZfilRhl+qVHjtut6b5LvJ9nb3a7ut0xJ0zbKbP8dwN8C/3DS+K1V9YGpV7TkJp3VB8jG1f9sdeTIxD93mFOZ2X/qLav/MnDOg08NX3nD6uPOsb1fH3lbQy/gAp7qfQrGbdclacFN8pn/xiQPdR8L7NIrLZhxw/9h4GXAduAA8MG1VrRXnzSfxgp/VR2sqqNVdQz4CHDF86xrrz5pDo11em+Sbce79AJvAB5+vvU1XZNO7vV1ivHZO/9r1dixvibg1vi5G7ZsWV3DT3/aTw0L7gXD37XrejVwbpL9wHuAVyfZDhTwBPC2HmuU1INx23V9tIdaJM2QZ/hJjTL8UqMMv9QoL+bRoFOZ1T+lU4nn4NTahZrZ37AyfPzY0dlsfiZbkTR3DL/UKMMvNcrwS41a2gm/bNo8dLyee3bGlSy2vq4TIGY2sbcWj/xSowy/1CjDLzXK8EuNMvxSo5Z2tn/hZvWHXY12Dk6X1fLyyC81yvBLjTL8UqNGadd1cZL7kuxL8kiSt3fjW5PsSvJod++1+6UFMsqE3xHgnVX1YJIzgC8n2QW8BdhdVe9LchNwE/Cu/kpdcjOc3Fs595yh4/c+tHvV2Ot/dnvf5WidjNKu60BVPdg9fgbYB1wIXAPs7FbbCVzbV5GSpu+UPvMneSnwSmAPcMHxa/d39+dPuzhJ/Rk5/EleDHwKeEdV/fgUXme7LmkOjRT+JJsYBP9jVfXpbvhgkm3d89uAQ8Nea7suaT6N0rEnDJp07KuqD53w1D3A9cD7uvu7e6lQU3fkFy8aOv5rN//JqrEX/f6xVWOnf2rP1Gs6VSu/cMnQ8aOPfXv1oGdPDjXKbP+rgD8EvpZkbzf2bgahvyvJDcB3gTf1U6KkPozSrusLwJD/OgF4zXTLkTQrnuEnNcrwS40y/FKjlvb7/Ettwtnr584cfmXjC+76xqqxo089NfLPndTKOVuHjg9rLzZ0Vn8tzuwP5ZFfapThlxpl+KVGGX6pUUsx4ZfTVn9noA4v8ZeIJpzA2vzZ+4eOT9o86uCf/daqsWfPHL7uxbf85+rtD5nYU3888kuNMvxSowy/1CjDLzXK8EuNWorZ/qWe2V8gF/zN6hl8zS+P/FKjDL/UKMMvNWqSdl3vTfL9JHu729X9lytpWiZp1wVwa1V9oL/yJPVllAt4HgCOd+Z5Jsnxdl2SFtgk7boAbkzyUJLb7dIrLZZJ2nV9GHgZsJ3BO4MPrvE623VJc2jsdl1VdbCqjlbVMeAjwBXDXmu7Lmk+jTLbP7Rd1/E+fZ03AA9PvzxJfZmkXdd1SbYDBTwBvK2XCiX1YpJ2XfdOvxxJs+IZflKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UqFEu4LklyZeSfLVr13VzN35Jkj1JHk3yiSSb+y9X0rSMcuQ/DFxVVa9gcI3+HUmuBN7PoF3XpcBTwA39lSlp2l4w/DXwk25xU3cr4Crgk934TuDaXiqU1ItRm3asdJftPgTsAr4FPF1VR7pV9mP/PmmhjBT+rjPPduAiBp15Lhu22rDX2q5Lmk+nNNtfVU8DnwOuBM5Kcvy6/xcBT67xGtt1SXNolNn+85Kc1T1+EfBaYB9wH/DGbrXrgbv7KlLS9I3SrmsbsDPJCoP/LO6qqn9O8nXgziR/BXyFQT8/SQtilHZdDwGvHDL+OGt05pU0/zzDT2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2rUKKf3Sutrw8rqsWNHZ1/HtA3bL5jZvnnklxpl+KVGGX6pUYZfapQTfpp/yzC5N8w675dHfqlRhl9qlOGXGmX4pUZN0qvvjiTfTrK3u23vv1xJ0zLKbP/xXn0/SbIJ+EKSf+me+/Oq+uTzvFbSnBrl6r0FDOvVJ2mBjdWrr6r2dE/dkuShJLcmGdqOx3Zd0nwaq1dfkl8B/gL4ZeDXga3Au9Z4re26pDk0bq++HVV1oGvffRj4e2zgIS2UcXv1fSPJtm4swLXAw30WKmm6JunV9+9JzgMC7AX+uMc6JU3ZJL36ruqlIkkz4Rl+UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81KoOGPDPaWPID4Dvd4rnAD2e28dlxvxbPMu3bS6rqvFFWnGn4/9+Gkweq6vJ12XiP3K/Fs8z79nx82y81yvBLjVrP8N+2jtvuk/u1eJZ539a0bp/5Ja0v3/ZLjZp5+JPsSPLNJI8luWnW25+mJLcnOZTk4RPGtibZleTR7v7s9axxHEkuTnJfkn1JHkny9m58ofctyZYkX0ry1W6/bu7GL0myp9uvTyTZvN61zsJMw981+/w74PeAlwPXJXn5LGuYsjuAHSeN3QTsrqpLgd3d8qI5Aryzqi4DrgT+tPt3WvR9OwxcVVWvALYDO5JcCbwfuLXbr6eAG9axxpmZ9ZH/CuCxqnq8qp4F7gSumXENU1NVnwd+dNLwNcDO7vFOBu3LF0pVHaiqB7vHzwD7gAtZ8H2rgZ90i5u6WwFXAZ/sxhduv8Y16/BfCHzvhOX93dgyuaCqDsAgRMD561zPRJK8lEGX5j0swb4lWUmyFzgE7AK+BTxdVUe6VZbxd3KoWYc/Q8b8c8OcSvJi4FPAO6rqx+tdzzRU1dGq2g5cxOCd6GXDVpttVetj1uHfD1x8wvJFwJMzrqFvB5NsA+juD61zPWNJsolB8D9WVZ/uhpdi3wCq6mngcwzmNM5KsrF7ahl/J4eadfjvBy7tZlc3A28G7plxDX27B7i+e3w9cPc61jKWJAE+Cuyrqg+d8NRC71uS85Kc1T1+EfBaBvMZ9wFv7FZbuP0a18xP8klyNfDXwApwe1XdMtMCpijJx4FXM/hW2EHgPcA/AXcBPwd8F3hTVZ08KTjXkvw28B/A14Bj3fC7GXzuX9h9S/KrDCb0Vhgc+O6qqr9M8vMMJp+3Al8B/qCqDq9fpbPhGX5SozzDT2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVH/B2Gm6iMSX3FMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(10)  # put any number here you want to see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maxpooling\n",
    "\n",
    "<img src=\"imgs/maxpool_animation.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "front.png  maxpool_animation.gif  side.png\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 38, 38, 32)        320       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 46208)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 46208)             184832    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 50)                2310450   \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,498,654\n",
      "Trainable params: 2,406,038\n",
      "Non-trainable params: 92,616\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 19s 475us/step - loss: 0.3249 - acc: 0.8614 - val_loss: 1.2934 - val_acc: 0.4951\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 17s 430us/step - loss: 0.2937 - acc: 0.8748 - val_loss: 1.6997 - val_acc: 0.4952\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 17s 425us/step - loss: 0.2844 - acc: 0.8800 - val_loss: 0.2747 - val_acc: 0.8836\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 17s 432us/step - loss: 0.2830 - acc: 0.8819 - val_loss: 0.8007 - val_acc: 0.6815\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 19s 481us/step - loss: 0.2810 - acc: 0.8832 - val_loss: 0.7066 - val_acc: 0.6351\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 17s 430us/step - loss: 0.2760 - acc: 0.8833 - val_loss: 0.3713 - val_acc: 0.8457\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 18s 451us/step - loss: 0.2797 - acc: 0.8835 - val_loss: 0.3968 - val_acc: 0.8404\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 18s 450us/step - loss: 0.2687 - acc: 0.8880 - val_loss: 0.3773 - val_acc: 0.8455\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 18s 440us/step - loss: 0.2686 - acc: 0.8906 - val_loss: 1.1057 - val_acc: 0.5223\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 18s 459us/step - loss: 0.2683 - acc: 0.8893 - val_loss: 0.3078 - val_acc: 0.8721\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 18s 439us/step - loss: 0.2681 - acc: 0.8898 - val_loss: 0.3321 - val_acc: 0.8563\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 17s 422us/step - loss: 0.2673 - acc: 0.8894 - val_loss: 2.1682 - val_acc: 0.6478\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 18s 462us/step - loss: 0.2639 - acc: 0.8918 - val_loss: 0.6047 - val_acc: 0.7514\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 19s 469us/step - loss: 0.2647 - acc: 0.8904 - val_loss: 0.2740 - val_acc: 0.8843\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 19s 465us/step - loss: 0.2649 - acc: 0.8921 - val_loss: 0.3933 - val_acc: 0.8382\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 18s 454us/step - loss: 0.2635 - acc: 0.8930 - val_loss: 0.6559 - val_acc: 0.6971\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 18s 456us/step - loss: 0.2631 - acc: 0.8924 - val_loss: 0.5116 - val_acc: 0.7832\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 18s 455us/step - loss: 0.2659 - acc: 0.8906 - val_loss: 0.4133 - val_acc: 0.8143\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 18s 455us/step - loss: 0.2619 - acc: 0.8952 - val_loss: 0.2930 - val_acc: 0.8803\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 18s 459us/step - loss: 0.2583 - acc: 0.8948 - val_loss: 0.4185 - val_acc: 0.8303\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 18s 457us/step - loss: 0.2617 - acc: 0.8936 - val_loss: 0.3017 - val_acc: 0.8725\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 18s 451us/step - loss: 0.2588 - acc: 0.8948 - val_loss: 0.3345 - val_acc: 0.8656\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 18s 454us/step - loss: 0.2551 - acc: 0.8950 - val_loss: 0.7918 - val_acc: 0.7636\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 18s 454us/step - loss: 0.2592 - acc: 0.8935 - val_loss: 0.7207 - val_acc: 0.6913\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 18s 451us/step - loss: 0.2531 - acc: 0.8974 - val_loss: 0.3055 - val_acc: 0.8789\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 18s 461us/step - loss: 0.2543 - acc: 0.8964 - val_loss: 0.3024 - val_acc: 0.8769\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 18s 454us/step - loss: 0.2572 - acc: 0.8950 - val_loss: 0.2884 - val_acc: 0.8784\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 18s 456us/step - loss: 0.2603 - acc: 0.8948 - val_loss: 0.5408 - val_acc: 0.7407\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 18s 454us/step - loss: 0.2562 - acc: 0.8980 - val_loss: 0.7997 - val_acc: 0.7360\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 18s 455us/step - loss: 0.2561 - acc: 0.8961 - val_loss: 0.3409 - val_acc: 0.8728\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 18s 452us/step - loss: 0.2592 - acc: 0.8965 - val_loss: 0.2970 - val_acc: 0.8762\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 18s 454us/step - loss: 0.2565 - acc: 0.8971 - val_loss: 0.4474 - val_acc: 0.8313\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 18s 456us/step - loss: 0.2640 - acc: 0.8929 - val_loss: 0.4306 - val_acc: 0.8427\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 18s 454us/step - loss: 0.2589 - acc: 0.8953 - val_loss: 0.2891 - val_acc: 0.8806\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 18s 454us/step - loss: 0.2693 - acc: 0.8905 - val_loss: 0.2889 - val_acc: 0.8792\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 18s 453us/step - loss: 0.2696 - acc: 0.8908 - val_loss: 5.0149 - val_acc: 0.5052\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 18s 457us/step - loss: 0.2607 - acc: 0.8940 - val_loss: 0.3645 - val_acc: 0.8500\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 18s 458us/step - loss: 0.2601 - acc: 0.8954 - val_loss: 0.8955 - val_acc: 0.7568\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 18s 456us/step - loss: 0.2533 - acc: 0.8987 - val_loss: 1.0113 - val_acc: 0.5543\n",
      "Epoch 40/100\n",
      "25248/40000 [=================>............] - ETA: 6s - loss: 0.2547 - acc: 0.8989"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f4812cfa11b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_signal_new\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/big_data36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/big_data36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/big_data36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/big_data36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/big_data36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the network\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=3, activation='tanh', \n",
    "          input_shape=(40,40,1),data_format = \"channels_last\"))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(50, activation='tanh'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(50, activation='tanh'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "print(model.summary())\n",
    "                                                    \n",
    "# Train the network\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "model.fit(images_train, keras.utils.to_categorical(df_train[\"is_signal_new\"]), epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Evaluate performance on independent sample\n",
    "# DO NOT CHANGE BELOW!\n",
    "\n",
    "# Prepare input\n",
    "store_test = pandas.HDFStore(\"test_without_truth_img_100k.h5\")\n",
    "df_test = store_test.select(\"table\")\n",
    "images_test = to_image(df_test)\n",
    "\n",
    "# Run DNN\n",
    "print(\"Running on full test sample. This may take a moment.\")\n",
    "ret = model.predict(images_test)\n",
    "np.save(\"result.npy\",ret[:,1])\n",
    "!zip result.zip result.npy\n",
    "print(\"Done. Click below  to download result\")\n",
    "FileLink('result.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

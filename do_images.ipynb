{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "In this tutorial, we will build a convolutional neural network. It is the second tutorial and contains a little less prewritten function calls i.e. to display the loss or to plot the roc. Use your knowledge from the first notebook and plot anything of interest!\n",
    "\n",
    "\n",
    "### What are CNNs?\n",
    "Compared to normal feedforward networks, we simply add an other type of layer. With images, we usually have\n",
    " - a large input space (width x length) \n",
    " - the **spatial position _matters_**: We would not recognize an image whose pixels are randomly shuffled!\n",
    " \n",
    "Convolution neural networks deal with this by using local information from several neighbouring nodes to learn about the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas, keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import FileLink, FileLinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get our data that is stored in the cloud. \n",
    "\n",
    "In case it doesn't work, use the alternative download by uncommenting the next cell.\n",
    "\n",
    "Using the link inside a browser, the data can also be downloaded to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the data from SWITCH\n",
    "!wget \"https://drive.switch.ch/index.php/s/xcLDFKJAyyZGmfD/download\" -O train_img.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative download from CERN, uncomment to use\n",
    "# !wget \"https://cernbox.cern.ch/index.php/s/LWuDUdgrKJ2vC5V/download?x-access-token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJkcm9wX29ubHkiOmZhbHNlLCJleHAiOiIyMDE5LTAyLTI2VDEzOjA5OjQwLjU2MDI1NDAwOSswMTowMCIsImV4cGlyZXMiOjAsImlkIjoiMTY1NzE2IiwiaXRlbV90eXBlIjowLCJtdGltZSI6MTU1MTE3OTM3NCwib3duZXIiOiJqZXNjaGxlIiwicGF0aCI6ImVvc2hvbWUtajo0NjQ3NjIzNjc4Mzg3ODE0NCIsInByb3RlY3RlZCI6ZmFsc2UsInJlYWRfb25seSI6dHJ1ZSwic2hhcmVfbmFtZSI6InRyYWluX2ltZy5oNSIsInRva2VuIjoiTFd1RFVkZ3JLSjJ2QzVWIn0.9iGjQkXtkr0TeQiO9WLbeHMdHIrhTP4_5l16Zz1ufNc\" -O train_img.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data and some dimensional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 image has 40x40 pixels = 1600 pixels\n",
    "pixels = [\"img_{0}\".format(i) for i in range(1600)]\n",
    "\n",
    "def to_image(df):\n",
    "    return  np.expand_dims(np.expand_dims(df[pixels], axis=-1).reshape(-1,40,40), axis=-1)\n",
    "\n",
    "\n",
    "# Read the first 10k events\n",
    "store_train = pandas.HDFStore(\"train_img.h5\")\n",
    "df_train = store_train.select(\"table\")\n",
    "images_train = to_image(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple helper function\n",
    "def plot_image(number):\n",
    "    plt.imshow(images_train[number, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the images that we gonna train on. Change the numbers and plot a gain to get an idea of what our input looks like. This is a gread advantage when using images as data: The visualization is straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(10)  # put any number here you want to see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer\n",
    "\n",
    "In simple terms, a convolutional layer is a filter/kernel (\"mini image\" consisting of 3x3 - 5x5 pixels) which scans over the whole image and returns a value at each position proportional to \"how well the mini image fits in this subset of pixels\". This, the how-well-it-fits, is the output of a convolutional layer, a \"feature map\". So for example, if the filter is a horizontal line but the subset of pixels is a vertical line, the output would be small, whereas if the subset were a horizontal line as well, the output would be large.\n",
    "\n",
    "The below animation illustrates this:\n",
    "\n",
    "<img src=\"imgs/convolution_animation.gif\">\n",
    "\n",
    "\n",
    "For a slightly more extended introduction and visualization see [here](https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks#convolution-layer)\n",
    "\n",
    "A crucial difference to the dense layer of (2 dimensional) convolutional layers is that their input is not a flat 1-D Tensor as before but a 2-D Tensor (it's an image in the end and, remember the spatial correlation that matters, we want to keep it as that). There are also 1 and 3 (and in principle N) dimensional convolutional layers (with 1-D or 3-D inputs respectively).\n",
    "\n",
    "Therefore we will need to change two things in our network compared to the previous one:\n",
    " - the input shape is different\n",
    " - since our output layer is 1-D, we need to flatten the 2-D structure somewhere\n",
    " \n",
    "**Syntax**:\n",
    "To create a convolutional layer in Keras, the syntax looks like this:\n",
    " \n",
    "```python\n",
    "keras.layers.Conv2D(number_of_filters, size_of_the_filter, activation_function)\n",
    "```\n",
    "where the number of filters controls the \"deepness\" of the representation and the size of the filter (`kernel_size`) specifies the dimension of the filter. A more detailed explanation can be found in the [keras convolutional layer docs](https://keras.io/layers/convolutional/) (make sure to scroll down to the Conv2D, not the Conv1D).\n",
    "\n",
    "**Exercise**:\n",
    "Add an additional hidden convolutional layer with 64 filters and a kernel size of 3 (and 'tanh' activation). Train it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 36, 36, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 73984)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 147970    \n",
      "=================================================================\n",
      "Total params: 167,298\n",
      "Trainable params: 167,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "activation = 'tanh'\n",
    "\n",
    "model_one_conv = keras.models.Sequential()\n",
    "model_one_conv.add(keras.layers.Conv2D(32, kernel_size=5, activation=activation, \n",
    "                                       input_shape=(40,40,1),  # again, only needed in the first layer. Here it's 2D!\n",
    "                                       data_format = \"channels_last\"))  # accounting: which of the dims is the \"color\"\n",
    "# replace this exact line by your layer\n",
    "model_one_conv.add(keras.layers.Conv2D(64, kernel_size=3, activation=activation))  # TODO: remove\n",
    "model_one_conv.add(keras.layers.Flatten())  # this is where the 2-dim structure gets flattened to 1-dim\n",
    "# we could also add a few Dense layer here, but let's do that later...\n",
    "model_one_conv.add(keras.layers.Dense(2, activation='softmax'))  # output layer\n",
    "\n",
    "model_one_conv.summary()  # print a summary of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the shape of the flatten layer: >70'000! That's a huge number of nodes (remember before, we had a couple of hundreds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "45000/45000 [==============================] - 16s 346us/step - loss: 0.3596 - acc: 0.8452 - val_loss: 0.3624 - val_acc: 0.8450\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 13s 289us/step - loss: 0.3444 - acc: 0.8560 - val_loss: 0.3642 - val_acc: 0.8434\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 13s 289us/step - loss: 0.3414 - acc: 0.8566 - val_loss: 0.3565 - val_acc: 0.8474\n",
      "Epoch 4/10\n",
      "45000/45000 [==============================] - 13s 288us/step - loss: 0.3424 - acc: 0.8562 - val_loss: 0.3532 - val_acc: 0.8514\n",
      "Epoch 5/10\n",
      "45000/45000 [==============================] - 13s 286us/step - loss: 0.3392 - acc: 0.8567 - val_loss: 0.3923 - val_acc: 0.8380\n",
      "Epoch 6/10\n",
      "45000/45000 [==============================] - 13s 287us/step - loss: 0.3377 - acc: 0.8568 - val_loss: 0.3565 - val_acc: 0.8430\n",
      "Epoch 7/10\n",
      "45000/45000 [==============================] - 13s 287us/step - loss: 0.3382 - acc: 0.8587 - val_loss: 0.3570 - val_acc: 0.8460\n",
      "Epoch 8/10\n",
      "45000/45000 [==============================] - 13s 291us/step - loss: 0.3377 - acc: 0.8575 - val_loss: 0.3625 - val_acc: 0.8388\n",
      "Epoch 9/10\n",
      "45000/45000 [==============================] - 13s 291us/step - loss: 0.3350 - acc: 0.8582 - val_loss: 0.3555 - val_acc: 0.8480\n",
      "Epoch 10/10\n",
      "45000/45000 [==============================] - 13s 291us/step - loss: 0.3369 - acc: 0.8582 - val_loss: 0.3526 - val_acc: 0.8502\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "model_one_conv.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "model_history = model_one_conv.fit(images_train, \n",
    "                                   keras.utils.to_categorical(df_train[\"is_signal_new\"]),\n",
    "                                   epochs=10,\n",
    "                                   validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maxpooling\n",
    "\n",
    "As we've noticed, the network hardly learned anything. But given the huge number of nodes, this does not really come in as a big surprise. On the other hand, we actually don't need _all_ of the pixels to get information from an image and we somehow should compress the image. The common way to do that is to do pooling. In simple terms, this is a downsampling by combining several (4-20 neighbouring) nodes (\"pixels\") to one. There are several ways of combining the nodes like taking the average of all nodes or the maximum.\n",
    "\n",
    "In the following, we will use the maximum, so called \"maxpooling\". The simple animation below shows how a (2, 2) shaped maxpooling works:\n",
    "\n",
    "<img src=\"imgs/maxpool_animation.gif\">\n",
    "\n",
    "**Syntax**:\n",
    "```python\n",
    "keras.layers.MaxPool2D(shape_of_the_pool)  # shape should be in this case a tuple like (int, int)\n",
    "```\n",
    "**Exercise**:\n",
    "Add a maxpooling layer between the second convolutional layer and the flattening. Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 36, 36, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 15490     \n",
      "=================================================================\n",
      "Total params: 34,818\n",
      "Trainable params: 34,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "activation = 'tanh'\n",
    "\n",
    "model_conv_pool = keras.models.Sequential()\n",
    "model_conv_pool.add(keras.layers.Conv2D(32, kernel_size=5, activation=activation, \n",
    "          input_shape=(40,40,1), data_format = \"channels_last\"))\n",
    "model_conv_pool.add(keras.layers.Conv2D(64, kernel_size=3, activation=activation))\n",
    "model_conv_pool.add(keras.layers.MaxPool2D(pool_size=(3, 3)))\n",
    "model_conv_pool.add(keras.layers.Flatten())\n",
    "model_conv_pool.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model_conv_pool.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "45000/45000 [==============================] - 12s 267us/step - loss: 0.3520 - acc: 0.8507 - val_loss: 0.3354 - val_acc: 0.8550\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 12s 265us/step - loss: 0.3180 - acc: 0.8646 - val_loss: 0.3253 - val_acc: 0.8634\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 12s 261us/step - loss: 0.3033 - acc: 0.8713 - val_loss: 0.3148 - val_acc: 0.8626\n",
      "Epoch 4/10\n",
      "45000/45000 [==============================] - 12s 262us/step - loss: 0.2874 - acc: 0.8790 - val_loss: 0.2948 - val_acc: 0.8746\n",
      "Epoch 5/10\n",
      "45000/45000 [==============================] - 12s 263us/step - loss: 0.2778 - acc: 0.8809 - val_loss: 0.2950 - val_acc: 0.8724\n",
      "Epoch 6/10\n",
      "45000/45000 [==============================] - 12s 265us/step - loss: 0.2722 - acc: 0.8850 - val_loss: 0.2962 - val_acc: 0.8700\n",
      "Epoch 7/10\n",
      "45000/45000 [==============================] - 12s 262us/step - loss: 0.2687 - acc: 0.8870 - val_loss: 0.2879 - val_acc: 0.8752\n",
      "Epoch 8/10\n",
      "45000/45000 [==============================] - 12s 260us/step - loss: 0.2657 - acc: 0.8887 - val_loss: 0.2942 - val_acc: 0.8754\n",
      "Epoch 9/10\n",
      "45000/45000 [==============================] - 12s 262us/step - loss: 0.2638 - acc: 0.8888 - val_loss: 0.2803 - val_acc: 0.8784\n",
      "Epoch 10/10\n",
      "45000/45000 [==============================] - 12s 258us/step - loss: 0.2600 - acc: 0.8906 - val_loss: 0.2846 - val_acc: 0.8806\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "model_conv_pool.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "model_history = model_conv_pool.fit(images_train, keras.utils.to_categorical(df_train[\"is_signal_new\"]), \n",
    "                                    epochs=10, \n",
    "                                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying such a \"primitive operation\" as taking the maximum of a few neighbouring pixels and combining them this way (which leaves us somehow with \"less\" information), we were able to increase the accuracy. A quite surprising result!\n",
    "\n",
    "We could have additionally added of course also a maxpooling _between_ the two convolutional layers. If you are interested, change the model above accordingly and train it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "We have stumbled upon quite a few concepts in this two tutorials. Now it is up to you to build a model and tweak it's parameters!\n",
    "\n",
    "The below \"pseudomodel\" can be used as a guide, but replace all `TODO` first, otherwise it won't work. For inspiration of network architecture, search through the internet and let yourself inspire from other well performing architectures (\"tutorial-like\", not state-of-the-art huge networks with dozens of layers from the tech giants!).\n",
    "\n",
    "Don't forget that `BatchNormalization` (at the end of the previous tutorial) between each layer can be a great way to improve training!\n",
    "\n",
    "**Exercise**:\n",
    "Play around! Tweak all the parameters we discussed, build any architecture you like and get an intuition for what affects what. Maybe also go back again and try some networks from the tutorials to see the specific effects certain layers can have.\n",
    "\n",
    "**Goal**\n",
    "Try to maximize the `val_acc`, the validation accuracy, and **keep your notebook with the maximum value for comparison in the next lecture.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'tanh'\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(TODO,  # number of filters\n",
    "                              kernel_size=TODO,  # kernel size, an int or (int, int)\n",
    "                              activation=activation, \n",
    "                              input_shape=(40,40,1),\n",
    "                              data_format = \"channels_last\"))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=TODO))  # a tuple like (3, 3)\n",
    "# another conv layer?\n",
    "# another maxpool layer?\n",
    "# probably more?\n",
    "model.add(keras.layers.Flatten())\n",
    "# a few dense layers here, probably dropout, probably a regularization?\n",
    "model_conv_pool.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model_conv_pool.summary()\n",
    "\n",
    "\n",
    "# Train the network\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics = [\"accuracy\"])\n",
    "model_history = model.fit(images_train, keras.utils.to_categorical(df_train[\"is_signal_new\"]),\n",
    "                          epochs=TODO,  # how many epochs to train on? 10-300 is a good range\n",
    "                          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you could plot the loss etc from the history. Don't remember how? Go back to the first tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: activation functions\n",
    "\n",
    "One topic that was not covered, are activation functions. If you are interested and already familiarized with the previous part, this is for you.\n",
    "\n",
    "The input into each node is fed into the activation function. An activation function has to be non-linear. There are a few often used activation functions:\n",
    " - tanh\n",
    " - sigmoid\n",
    " - relu (and friends: relu6, leakyRelu, selu,...)\n",
    " \n",
    "Throughout the tutorials we used `tanh`, which is an more classic activation function that can have problems with deeper networks ([vanishing gradient](https://medium.com/@anishsingh20/the-vanishing-gradient-problem-48ae7f501257)).\n",
    "\n",
    "There are advantages and disadvantages to all of the above functions. Feel free to change in the examples the activation functions (make sure to change _all_ of them in a single example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

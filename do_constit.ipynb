{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas, keras\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from IPython.display import FileLink, FileLinks\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (20.0, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the right amount of resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.15\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the dataset:\n",
    "\n",
    "## Load the dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_per_event=40\n",
    "n_events=10000\n",
    "features=4\n",
    "\n",
    "#Load the dataset from storage\n",
    "store_train = pandas.HDFStore(\"/disk/lhcb_data/davide/ML4HEP_exercise/train.h5\")\n",
    "df_train = store_train.select(\"table\",stop=n_events)\n",
    "\n",
    "#Define a list with the desired kinematic variables to access the dataset\n",
    "cols = [c.format(i) for i in range(particles_per_event) for c in [\"E_{0}\",  \"PX_{0}\",  \"PY_{0}\",  \"PZ_{0}\"]]\n",
    "\n",
    "#Extract the train set and the training labels\n",
    "T = df_train[cols].values[0:n_events].reshape(n_events,particles_per_event,features)\n",
    "T_labels=df_train[\"is_signal_new\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset tensor\n",
    "Our dataset is organized in a rank 3 tensor T, whose elements are labeled by indices (i, j, k). Index i runs on the event number, j numbers the particles in each event and index k numbers the variable associated to the track in the event\n",
    "\n",
    "$$\n",
    "T = (i=1,...,n_{events}, j=1,...,\\textit{particles per event}, k= E_{j} , PX_{j}, PY_{j}, PZ_{j})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancy indexing\n",
    "\n",
    "For example, we can access to the single particles four momentum in an event by making use of indexing. Let's take the first particle in the first event: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first particle in the first event has the following four momentum \\n')\n",
    "print('E_0 = {0:.5g}'.format(T[0,0,0]))\n",
    "print('PX_0 = {0:.5g}'.format(T[0,0,1]))\n",
    "print('PY_0 = {0:.5g}'.format(T[0,0,2]))\n",
    "print('PZ_0 = {0:.5g}'.format(T[0,0,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same information can be accessed using numpy fancy indexing syntax, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first particle in the first event has the following four momentum \\n')\n",
    "print(T[0,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:Now, using numpy array methods calculate:\n",
    "\n",
    "1) the mean energy of all particles over all events (hint: exclude from the mean the empty events)\n",
    "\n",
    "\n",
    "2) the mean energy of the first 5 particles for the first 10 events\n",
    "\n",
    "3) the maximum PZ (in modulus) of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution:\n",
    "\n",
    "#1) T[:,:,0][np.where(T[:,:,0]!=0)].mean()\n",
    "#2) T[0:10,0:5,0].mean()\n",
    "#3) np.abs(T[:,:,3]).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables at hadron colliders: kinematic invariants\n",
    "\n",
    "\n",
    "The dataset can be cast to the $(E \\text{, } \\phi \\text{, } \\eta)$ space.\n",
    "The rapidity $\\eta$ is related to $PZ$ of the particle track. It is defined as:\n",
    "\n",
    "$$\n",
    "\\eta = \\frac{1}{2} ln \\left(\\frac{E - PZ}{ E + PZ} \\right)\n",
    "$$\n",
    "\n",
    "While $\\phi$ is related to the $(PX, PY)$ components by:\n",
    "\n",
    "$$\n",
    "\\phi = arctg\\left( \\frac{PY}{PX} \\right)\n",
    "$$\n",
    "\n",
    "As shown in figure, the CMS detector is formed by:\n",
    "\n",
    "1) a cylindrical barrel detector placed around the interaction point that covers a rapidity range of $|\\eta| < 2.5 $\n",
    "\n",
    "2) two $1.48 < | \\eta | < 3.0$ endcap regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td><img src=\"imgs/front.png\" style=\"width:500px;height:400px;\">\n",
    "<td><img src=\"imgs/side.png\" style=\"width:500px;height:400px;\">\n",
    "<tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<caption><u><font color='purple'>**Figure**</u><font color='purple'>Need a caption here<br> <font color='black'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting events\n",
    "\n",
    "Each event can be displayed as 2D image in the ($\\phi, \\eta$) coordinates, whose pixels values are the corresponding E recorded by the calorimeter.\n",
    "\n",
    "First can convert the dataset to this new set of variables. We can do it with a few lines of code thanks to indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta=np.where(T[:,:,0]==0, 0, 0.5*np.log((T[:,:,0]-T[:,:,3])/(T[:,:,0]+T[:,:,3])));\n",
    "phi=np.where(T[:,:,0]==0, 0, np.arctan(T[:,:,2]/T[:,:,1]));\n",
    "E=T[:,:,0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can check that the $\\phi$ angular coverage is of $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_range=np.abs(phi.min())+phi.max()\n",
    "print(phi_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that the rapidity range is within the expected one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_range=np.abs(eta.min())+eta.max()\n",
    "eta.min(), eta.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to draw each event we rescale the dataset to be compatible with the image representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_pixels=eta_pixels=40\n",
    "eta_rescaled=(eta+np.abs(eta.min()))/eta_range*eta_pixels\n",
    "phi_rescaled=(phi+np.abs(phi.min()))/phi_range*phi_pixels\n",
    "\n",
    "\n",
    "pic = np.zeros(shape=(phi_pixels,eta_pixels,1),dtype=np.float32)\n",
    "pics=np.array([pic for j in range(0,n_events)])\n",
    "    \n",
    "for event in range(n_events):\n",
    "\n",
    "    for n_track in range((phi[event]>0).sum()):\n",
    "        #print(n_track)\n",
    "        phi_coord = int(np.floor(phi_rescaled[event][n_track]))-1\n",
    "        eta_coord = int(np.floor(eta_rescaled[event][n_track]))-1\n",
    "            \n",
    "        pics[event,phi_coord,eta_coord]=E[event][n_track]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_display=100\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(pics[event_display].reshape(40,40))\n",
    "plt.xlabel('eta', fontsize=15)\n",
    "plt.ylabel('phi', fontsize=15)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('E (GeV)')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(E[np.where(2<E)],range=(0,250), bins=50);\n",
    "plt.xlabel('E (GeV)', fontsize=15)\n",
    "plt.ylabel('dN/dE', fontsize=15)\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(20,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a dense Deep Neural Network\n",
    "\n",
    "## Adding more layers\n",
    "\n",
    "Visualising the dataset with images helps to familiarize with the dataset, but in the following example we will use the jet constituents representation. Networks dealing with rank 1 inputs are called dense networks.\n",
    "\n",
    "We provide you with the syntax that builds a simple feed forward dense Artificial Neural Network, you should make it deep and add 5 layers of 200 nodes each. The syntax for adding one intermediate layer in keras is the following:\n",
    "\n",
    "```python\n",
    "model.add(keras.layers.Dense(number_of_nodes, activation_function)\n",
    "```\n",
    "\n",
    "(The keras documentation site is a useful resource if you're stuck at any time: https://keras.io/)\n",
    "\n",
    "***Exercise***:\n",
    "As an example add 5 layers, each one containing 200 nodes.\n",
    "\n",
    "***Remember***: this is an example and the number of output nodes per layer can be chosen arbitrarily, usally, a good practice is to reduce the number of output nodes with increasing layer number i.e. layer 1 # of output nodes = 200, layer 2 # of output nodes 100 etc.. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Syntax that creates the network\n",
    "\n",
    "\"\"\"\n",
    "#Creates the sequential model\n",
    "model = keras.models.Sequential() \n",
    "nodes=200\n",
    "number_of_classes=2\n",
    "\n",
    "#########\n",
    "model.add(keras.layers.Dense(nodes,                                          # number of output nodes for layer\n",
    "                             \n",
    "                             input_shape = (particles_per_event*features,),  # important:\n",
    "                                                                             # in the first layer you have to \n",
    "                                                                             # specify the input shape which\n",
    "                                                                             # depends on your data\n",
    "                             \n",
    "                             activation='tanh'))                             # layer's activation function\n",
    "\n",
    "model.add(keras.layers.Dense(number_of_classes,                              # number of classes, in our case 2\n",
    "                                                                             # either signal or background\n",
    "                             \n",
    "                             activation='softmax'))                          # activation layer \n",
    "\n",
    "#########\n",
    "\n",
    "\"\"\"\n",
    "Syntax that trains the network\n",
    "\n",
    "\"\"\"\n",
    "model.compile(loss='categorical_crossentropy', # Loss used for this model\n",
    "              \n",
    "              optimizer=\"Adam\",                # Choice of optimizer algorithm\n",
    "              \n",
    "              metrics = [\"accuracy\"])          # a way to measure the performance\n",
    "                                               # of your network\n",
    "\n",
    "model_history = model.fit(x=T.reshape(n_events,particles_per_event*features), # train dataset\n",
    "                          \n",
    "                          y=keras.utils.to_categorical(T_labels),             # training labels\n",
    "                          \n",
    "                          validation_split = 0.1,                             # fraction of the dataset that \n",
    "                                                                              # will be used as validation set\n",
    "                          \n",
    "                          batch_size=128,                                     # fraction of the dataset that\n",
    "                                                                              # the network sees in a cycle of \n",
    "                                                                              # forward and backward propagation\n",
    "                          \n",
    "                          verbose=1,                                          # set the verbosity during training\n",
    "                                                                              # 0 = silent, 1 = progress bar, \n",
    "                                                                              # 2 = one line per epoch.\n",
    "                          \n",
    "                          epochs=300)                                         # number of times the network sees \n",
    "                                                                              # the entire dataset\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model.fit() method outputs a history object whose keys are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Exercise***:\n",
    "Using the train-validation splitting of the train set, show the over-fitting effect of a high capacity network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss on train set')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model_history.history['acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy on train set')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss on validation set')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model_history.history['val_acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy on validation set')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Find the optimal network capacity\n",
    "\n",
    "Deeper network allows to better approximate more complicated problems, but it may lead to overfitting effects for less complicated problems, in other words many layers and many nodes doesn't always mean better performances.\n",
    "\n",
    "The ***first step*** in building a good network is finding the right architecture, with the right capacity.  \n",
    "In the next exercise we suggest to create a new network with a ***reduced number of nodes***  and ***reduced number of layers*** with respect to the previous example.\n",
    "\n",
    "An ***indication*** to estimate the capacity of the network is the total number of trainable parameters, which you can read off by adding \n",
    "\n",
    "```python\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "***Exercise***\n",
    "Find an optimal architecture by reducing the number of nodes and the number of layers. \n",
    "Does it help tackling the overfitting problem? \n",
    "\n",
    "(hint: we suggest to reduce the number of nodes per layer to 10 and reduce the number of layers to 3 intermediate layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Syntax that creates the network\n",
    "\n",
    "\"\"\"\n",
    "#Creates the sequential model\n",
    "model = keras.models.Sequential() \n",
    "nodes=10\n",
    "number_of_classes=2\n",
    "\n",
    "####\n",
    "\n",
    "#Add layers here\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Syntax that trains the network\n",
    "\n",
    "\"\"\"\n",
    "model.compile(loss='categorical_crossentropy', \\\n",
    "              optimizer=\"Adam\", \\\n",
    "              metrics = [\"accuracy\"]) \n",
    "\n",
    "model.summary()\n",
    "model_history = model.fit(x=T.reshape(n_events,particles_per_event*features), #\n",
    "                          y=keras.utils.to_categorical(T_labels),             #\n",
    "                          validation_split = 0.1,                             #\n",
    "                          batch_size=128,                                     #\n",
    "                          verbose=0,                                          #\n",
    "                          epochs=300)                                         #\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss on train set')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model_history.history['acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy on train set')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss on validation set')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model_history.history['val_acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy on validation set')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network optimization methods\n",
    "\n",
    "Sometimes though you may need a network with more trainable parameters and, in order to prevent the overfitting tendency of a high capacity network, several regularization methods are available\n",
    "\n",
    "These methods are of great importance but must not be abused since they introduce an external bias during the training procedure.\n",
    "\n",
    "## 1) L2 regularization\n",
    "\n",
    "\n",
    "L2 regularization adds a penalty term for the loss which is proportional to the weights' values (L1) or weights' values squared (L2). This prevents some weights to become numerically dominant with respect to others. It consists of appropriately modifying your cost function, from:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  \\hat{y}^{(i)}\\log\\left(y^{(i)}\\right) + (1-\\hat{y}^{(i)})\\log\\left(1- y^{(i)}\\right) \\large{)} \n",
    "$$\n",
    "To:\n",
    "$$\\mathcal{L}_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small \\hat{y}^{(i)}\\log\\left(y^{(i)}\\right) + (1-\\hat{y}^{(i)})\\log\\left(1- y^{(i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "where $\\hat{y}^{(i)}$, ($y^{(i)}$) is the label (network prediction) of the $i$-ith example, $m$ is the total number of examples in the train set and $\\lambda$ is the regularization weight. The index $l$ runs on the layer numbers and $W_{k,j}$ are the weight matrices of the network.\n",
    "\n",
    "In keras, the regularization term are applied on a per-layer basis through the keyword argument\n",
    "\n",
    "```python\n",
    "model.add(keras.layers.Dense(output_shape, kernel_regularizer=kernel_regularizer.l2(lambd), bias_regularizer=regularizers.l2(lamb), activation_function)\n",
    "```\n",
    "\n",
    "***Exercise*** \n",
    "Revert the network architecture to the high capacity case and use it to show the effect of adding L2 regularization on the loss function and on the train/validation accuracies. you can play with the hyperparameter $\\lambda$ (called ```lambd``` in code) and see the effect of stronger regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\"\"\"\n",
    "Syntax that creates the network\n",
    "\n",
    "\"\"\"\n",
    "#Creates the sequential model\n",
    "\n",
    "model_with_reg = keras.models.Sequential() \n",
    "lambd=0.01\n",
    "nodes=200\n",
    "##################\n",
    "\n",
    "\n",
    "#Place layers with L2 regularization here\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\"\"\"\n",
    "Syntax that trains the network\n",
    "\n",
    "\"\"\"\n",
    "model_with_reg.compile(loss='categorical_crossentropy', \\\n",
    "              optimizer=\"Adam\", \\\n",
    "              metrics = [\"accuracy\"]) \n",
    "\n",
    "model_with_reg.summary()\n",
    "model_with_reg_history = model_with_reg.fit(x=T.reshape(n_events,particles_per_event*features), \n",
    "                                            y=keras.utils.to_categorical(T_labels), \n",
    "                                            validation_split = 0.1, \n",
    "                                            batch_size=128, \n",
    "                                            verbose=1, \n",
    "                                            epochs=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_with_reg_history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss on train set, L2 regularization added')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model_with_reg_history.history['acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy on train set, L2 regularization added')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_with_reg_history.history['val_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss on validation set, L2 regularization added')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model_with_reg_history.history['val_acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy on validation set, L2 regularization added')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Dropout regularization\n",
    "\n",
    "Dropout regularization acts during training, it introduces a non-zero probability of switching some weights' values to zero. By doing this, the network is taught not to rely to heavily on one particular weight and helps preventing overfitting.\n",
    "\n",
    "Dropout is added right after the layer's definition you intend to apply it to. For example, after the first layer:\n",
    "\n",
    "```python\n",
    "model.add(keras.layers.Dense(nodes, input_shape = (particles_per_event*features,), activation='tanh'))\n",
    "model.add(keras.layers.Dropout(dropout_rate))\n",
    "```\n",
    "\n",
    "Note that the ***dropout doesn't add any trainable parameter*** to the network, as you can read directly with the ```model.summary()``` method\n",
    "\n",
    "***Exercise*** Test dropout regularization on the same network and draw the loss/accuracy plots, play with the ```dropout_rate``` and vary the average fraction of input units set to $0$ during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Syntax that creates the network\n",
    "\n",
    "\"\"\"\n",
    "#Creates the sequential model\n",
    "model_with_drop = keras.models.Sequential() \n",
    "dropout_rate=0.1\n",
    "nodes=200\n",
    "##################\n",
    "\n",
    "\n",
    "#Place layers with dropout regularization here\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\"\"\"\n",
    "Syntax that trains the network\n",
    "\n",
    "\"\"\"\n",
    "model_with_drop.compile(loss='categorical_crossentropy', \n",
    "              optimizer=\"Adam\", \n",
    "              metrics = [\"accuracy\"]) \n",
    "\n",
    "model_with_drop.summary()\n",
    "model_with_drop_history = model_with_drop.fit(x=T.reshape(n_events,particles_per_event*features), \n",
    "                                            y=keras.utils.to_categorical(T_labels), \n",
    "                                            validation_split = 0.1, \n",
    "                                            batch_size=128, \n",
    "                                            verbose=1, \n",
    "                                            epochs=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_with_drop_history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss on train set, dropout regularization added')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model_with_drop_history.history['acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy on train set, dropout regularization added')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_with_drop_history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss on train set, dropout regularization added')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model_with_drop_history.history['acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy on train set, dropout regularization added')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Adding batch-normalization\n",
    "\n",
    "Batch-normalization (BN) is a useful tool in training since it stabilizes the training process by normalising the weights at each batch to null mean and unit standard deviation. \n",
    "\n",
    "BN is applied to layers singularly and it is generally used to normalise the input to the activation function. It makes the weights of deep layers of the neural network more robust to change in weights of the first layers. By reducing the coupling between layers it causes each layer to learn more independently from the others and has a net effect of speeding up the learning process.\n",
    "\n",
    "As an example it is implemented for the first layer in the following way\n",
    "\n",
    "```python\n",
    "model.add(keras.layers.Dense(nodes, input_shape = (particles_per_event*features,))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "```\n",
    "In addition we can add a dropout probability with the line:\n",
    "\n",
    "```python\n",
    "model.add(keras.layers.Dropout(dropout_rate))\n",
    "```\n",
    "\n",
    "***Exercise*** The following neural network puts together all the techniques shown in the previous examples, feel free to edit and play with it to see the effect on the training process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "\"\"\"\n",
    "Syntax that creates the network\n",
    "\n",
    "\"\"\"\n",
    "#Creates the sequential model\n",
    "model_with_bn = keras.models.Sequential() \n",
    "dropout_rate=0.1\n",
    "lambd=0.08\n",
    "nodes=100\n",
    "##################\n",
    "\n",
    "\n",
    "model_with_bn.add(keras.layers.Dense(nodes*2, input_shape = (particles_per_event*features,)))\n",
    "model_with_bn.add(keras.layers.normalization.BatchNormalization())\n",
    "model_with_bn.add(keras.layers.LeakyReLU(0.1))\n",
    "model_with_bn.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "model_with_bn.add(keras.layers.Dense(nodes, kernel_regularizer=regularizers.l2(lambd), bias_regularizer=regularizers.l1(lambd),))\n",
    "model_with_bn.add(keras.layers.normalization.BatchNormalization())\n",
    "model_with_bn.add(keras.layers.LeakyReLU(0.1))\n",
    "model_with_bn.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "model_with_bn.add(keras.layers.Dense(nodes//2, kernel_regularizer=regularizers.l2(lambd), bias_regularizer=regularizers.l1(lambd),))\n",
    "model_with_bn.add(keras.layers.normalization.BatchNormalization()) \n",
    "model_with_bn.add(keras.layers.LeakyReLU(0.1))\n",
    "model_with_bn.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "model_with_bn.add(keras.layers.Dense(nodes//4, kernel_regularizer=regularizers.l2(lambd), bias_regularizer=regularizers.l1(lambd),))\n",
    "model_with_bn.add(keras.layers.normalization.BatchNormalization()) \n",
    "model_with_bn.add(keras.layers.LeakyReLU(0.1))\n",
    "model_with_bn.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "model_with_bn.add(keras.layers.Dense(nodes//10, kernel_regularizer=regularizers.l2(lambd), bias_regularizer=regularizers.l1(lambd),))\n",
    "model_with_bn.add(keras.layers.normalization.BatchNormalization()) \n",
    "model_with_bn.add(keras.layers.LeakyReLU(0.1))\n",
    "model_with_bn.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "model_with_bn.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "#################\n",
    "\n",
    "\"\"\"\n",
    "Syntax that trains the network\n",
    "\n",
    "\"\"\"\n",
    "model_with_bn.compile(loss='categorical_crossentropy', \n",
    "              optimizer=\"Adam\", \n",
    "              metrics = [\"accuracy\"]) \n",
    "\n",
    "model_with_bn.summary()\n",
    "model_with_bn_history = model_with_bn.fit(x=T.reshape(n_events,particles_per_event*features), \n",
    "                                            y=keras.utils.to_categorical(T_labels), \n",
    "                                            validation_split = 0.1, \n",
    "                                            batch_size=256, \n",
    "                                            verbose=1, \n",
    "                                            epochs=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_with_bn_history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss on train set, dropout regularization, batch-normalization')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model_with_bn_history.history['acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy on train set, dropout regularization, batch-normalization')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(model_with_bn_history.history['val_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss on train set, dropout regularization, batch-normalization')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model_with_bn_history.history['val_acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy on train set, dropout regularization, batch-normalization')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the network performance: ROC/AUC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Evaluate performance on independent sample\n",
    "# Prepare input\n",
    "store_test = pandas.HDFStore(\"/disk/lhcb_data/davide/ML4HEP_exercise/test.h5\")\n",
    "df_test = store_test.select(\"table\")\n",
    "\n",
    "test_size=2000\n",
    "\n",
    "\n",
    "T_test=df_test[cols].values[0:test_size].reshape(test_size,particles_per_event,features)\n",
    "T_test_labels=keras.utils.to_categorical(df_test[\"is_signal_new\"])[0:test_size]\n",
    "\n",
    "# Run DNN\n",
    "\n",
    "print(\"Running on full test sample for each model trained. This may take a moment.\")\n",
    "predictions_bn = model_with_bn.predict(T_test.reshape(test_size, particles_per_event*features))\n",
    "\n",
    "#Add ROC/AUC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_bn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = predictions_bn[:,1][np.where(T_test_labels[:,1]==1)]\n",
    "false_positives = predictions_bn[:,1][np.where(T_test_labels[:,0]==1)]\n",
    "\n",
    "plt.hist(true_positives,alpha=0.5,bins=80,density=True,label=\"True positives\");\n",
    "plt.hist(false_positives,alpha=0.5,bins=80,density=True, label=\"False positives\");\n",
    "plt.legend()\n",
    "plt.xlabel(\"NN output\", fontsize='15')\n",
    "plt.ylabel(\"Events (a.u.)\", fontsize='15')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(16,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_range=np.linspace(0.0,1.,num=30)\n",
    "\n",
    "sig_eps_vals=[sel_eff(true_positives,threshold_range[i]) for i in range(len(threshold_range))]\n",
    "bkg_eps_vals=[sel_eff(false_positives,threshold_range[i]) for i in range(len(threshold_range))]\n",
    "\n",
    "plt.plot(threshold_range,threshold_range, 'black', linestyle='dashed')\n",
    "plt.plot(bkg_eps_vals,sig_eps_vals, 'r', label=\"NN ROC Curve\")\n",
    "pAUC_NN=roc_auc_score(T_test_labels,predictions_bn)\n",
    "print(\"pAUC from NN {0}\".format(pAUC_NN))\n",
    "\n",
    "plt.plot(threshold_range,threshold_range, 'black', linestyle='dashed')\n",
    "\n",
    "plt.xlabel(\"Background rejection efficiency\", fontsize='15')\n",
    "plt.ylabel(\"Signal selection efficiency\", fontsize='15')\n",
    "\n",
    "plt.text(0.69,0.1,\"NN AUC {0:.4g}\\n\".format(pAUC_NN), bbox=dict(boxstyle=\"round\", facecolor='blue', alpha=0.10), horizontalalignment='center', verticalalignment='center',fontsize='15')\n",
    "plt.legend()\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(8,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
